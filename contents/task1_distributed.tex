\subsection{Distributed approach}
\label{subsec:task1-exp-distr}

\subsubsection{Experiment 1: fixed number of agents}
\label{subsubsec:task1-exp-distr-1}

The first group of experiments, summarised in Table \ref{tab:modeln5dist}, 
examines the behaviour of the control learned in the case of the three different 
inputs, \texttt{prox\_values}, \texttt{prox\_comm} or \texttt{all\_sensors}, for a 
number of robots $N$ and an \texttt{avg\_gap} both fixed at $5$ and the second 
chosen between $8$, $13$ and $24$.
\begin{figure}[!htb]
	\centering
	\begin{tabular}{cccc}
		\toprule
		\textbf{Model} \quad & \textbf{\texttt{network\_input}} & 
		\textbf{\texttt{input\_size}} &
		\textbf{\texttt{avg\_gap}} \\
		\midrule
		\texttt{net-d1} 				 & \texttt{prox\_values}	&  $  7$  &  $  8$  \\
		\texttt{net-d2} 				& \texttt{prox\_values}	    &  $  7$  &  $13$ \\
		\texttt{net-d3} 				& \texttt{prox\_values}	    &  $  7$  &  $24$  \\
		\texttt{net-d4} 				 & \texttt{prox\_comm}	  &  $  7$  &  $  8$  \\
		\texttt{net-d5} 				 & \texttt{prox\_comm}	  &  $  7$  &  $13$  \\
		\texttt{net-d6} 				 & \texttt{prox\_comm}	  &  $  7$  &  $24$  \\
		\texttt{net-d7} 				 & \texttt{all\_sensors}	  &  $14$  &  $  8$  \\
		\texttt{net-d8} 				 & \texttt{all\_sensors}	  &  $14$  &  $13$ 	\\
		\texttt{net-d9} 				 & \texttt{all\_sensors}	  &  $14$  &  $24$ 	\\
		\bottomrule
	\end{tabular}
	\captionof{table}[Experiments with $5$ agents (no communication).]{List of the 
	experiments carried out with $5$ agents.}
	\label{tab:modeln5dist}
\end{figure}

First of all we start by showing in Figure \ref{fig:distloss} an overview of the 
models performance in terms of train and validation losses. 
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.85\textwidth]{contents/images/task1/loss-distributed-all@}%
	\caption[Comparison of losses of the first set of 
	experiments.]{Comparison of 
	the losses of the models carried out with $5$ agents.}
	\label{fig:distloss}
\end{figure}
It is immediately evident that, in case of \texttt{prox\_values} inputs, the 
experiment performed with an \texttt{avg\_gap} of $24$ is not remarkable since 
the gap exceeds the maximal range of the sensor. In fact, from this analysis we 
generally expect a more stable behaviour using both types of input together, i.e.,
\texttt{all\_sensors}, as they are able to perform with both small and large gaps.

\paragraph*{Results using \texttt{prox\_values} input}
\label{para:1}
We start the analysis by exploring the results of the experiments obtained 
using the \texttt{prox\_values} readings alone as input of the network, continuing 
the with \texttt{prox\_comm} and concluding with \texttt{all\_sensors}.

The performance of \texttt{net-d1} are shown in the following images. In 
particular, in Figure \ref{fig:net-d1r2} is visualised a comparison of the \gls{r2}, 
or coefficient of determination, of the manual and the learned controllers, on the 
validation set.
This score function evidences how well the regression predictions approximate 
the real data points (groundtruth). Since a model which perfectly predicts the data 
has a score of $1$, we assume that a higher score corresponds to a model that 
performs better.
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d1/regression-manualvsomniscient}%
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d1/regression-net-d1-vs-omniscient}
	\end{subfigure}
	\caption[Evaluation of the \gls{r2} coefficients of \texttt{net-d1} 
	.]{Comparison of 
	the \gls{r2} coefficient of the manual and the controller learned from 
	\texttt{net-d1} with respect to the omniscient one.}
	\label{fig:net-d1r2}
\end{figure}
From these figures we expect that the robots' behaviour using the learned 
controller instead of the manual one is a bit better, even if far from the omniscient 
controller.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=.7\textwidth]{contents/images/net-d1/animation-distributed}%
	\caption[Evaluation of the trajectories obtained with \texttt{prox\_values} 
	input.]{Comparison of trajectories, of a single simulation, generated using three 
	controllers: the expert, the manual and the one learned from \texttt{net-d1}.}
	\label{fig:net-d1traj1}
\end{figure}
In Figure \ref{fig:net-d1traj1} we first show a sample simulation: on the y-axis is 
represented the position of each agent, while on the x-axis the simulation time 
steps. We compare the trajectories obtained from the three controllers, in 
particular visualising the omniscient one in blue, the manual in orange and the 
learned one in green.
The extreme robots are passive. The agents moved using the omniscient controller 
reach the target very quickly, in a couple of time steps. Those moved using the 
manual controller are slower, they approach the goal position on average in 10 
time steps but never reach it. Instead, the learned controller is even slower than 
the previous one, but in about 25 time steps, the agents manage to arrive in the 
correct final configuration. 

In Figure \ref{fig:net-d1traj} we show a comparison of the expert and the learned 
trajectories, and then between the manual and the learned ones, this time 
summarising the performance over all the validation runs: at each time step, the 
position of each agent is presented 
\begin{figure}[!htb]
	\begin{center}
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=.9\textwidth]{contents/images/net-d1/position-overtime-omniscient}%
			\caption{Expert controller trajectories.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=.9\textwidth]{contents/images/net-d1/position-overtime-learned_distributed}
			\caption{Distributed controller trajectories.}
		\end{subfigure}
	\end{center}
	\begin{center}
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=.9\textwidth]{contents/images/net-d1/position-overtime-manual}%
			\caption{Manual controller trajectories.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=.9\textwidth]{contents/images/net-d1/position-overtime-learned_distributed}
			\caption{Distributed controller trajectories.}
		\end{subfigure}
	\end{center}
	\caption[Evaluation of the trajectories learned by \texttt{net-d1}]{Comparison 
	of trajectories, of all the simulation runs, generated 
	using three controllers: the expert, the manual and 
		the one learned from \texttt{net-d1}.}
	\label{fig:net-d1traj}
\end{figure}

\noindent
as an average over all the simulation runs, and 
besides is shown this average minus and plus the standard deviation.
As expected, the convergence of the robots to the target using the omniscient 
controller is much faster than with the manual or the learned one. Generally, the 
learned trajectories require a higher number of time steps to converge to the 
correct configuration, sometimes even $40$ may be necessary, compared to the 
two other controllers that need less than $10$.

Indeed, analysing in Figure \ref{fig:net-d1control} the evolution of the control 
over time, it is possible to notice that the omniscient in the first time steps uses a 
higher speed than that chosen by the manual controller or the one predicted by 
the network. 
After about $10$ time steps the expert reaches the target while the manual need 
about $15$ time steps to arrive to the goal with a certain tolerance, maintaining 
then the speed constant at $0$. Instead, the distributed controller decreases the 
speed of the agents as the time steps pass, reaching zero speed but with a certain 
variance, probably caused by oscillations.
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d1/control-overtime-omniscient}%
		\caption{Expert controller.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d1/control-overtime-manual}%
		\caption{Manual controller.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d1/control-overtime-learned_distributed}
		\caption{Distributed controller.}
	\end{subfigure}
	\caption[Evaluation of the control learned by \texttt{net-d1}.]{Comparison 
		of output control decided using three controllers: the expert, the manual 
		and the one learned from \texttt{net-d1}.}
	\label{fig:net-d1control}
\end{figure}

In Figure \ref{fig:net-d1responsesensors} is visualised the response of the learned 
controller as the input sensing changes. 
In particular we analyse two cases. The first one shows the control predicted by 
the network when the robot sees only in front and nothing behind, more 
specifically when the given input is  $([0, 0, x, 0, 0, 0, 0])$, with $x$ varying in the 
range $[0, 4500]$.
The second shows the control predicted by the network when the robot instead 
sees nothing in front, more specifically when the given input is  $([0, 0, 0, 0, 0,x , 
x])$, with $x$ varying in the range $[0, 4500]$.
The behaviour is almost as expected. When the robot sees nothing behind but 
something in front, the model returns a negative speed, since the robot has to 
move backwards. 
The absolute value of control increases as the proximity to the obstacle increases.
A complementary behaviour is obtained when the robot sees only behind but 
not in front.
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d1/response-net-d1-front}%
		%\caption{response-net-d1-net([0, 0, x, 0, 0, 0, 0]).}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d1/response-net-d1-rear}
		%\caption{response-net-d1-net([0, 0, 0, 0, 0, x, x]).}
	\end{subfigure}
	\caption[Response of \texttt{net-d1} by varying the input sensing.]{Response of 
		\texttt{net-d1} by varying the input sensing.}
	\label{fig:net-d1responsesensors}
\end{figure}

In Figure \ref{fig:net-d1responseposition} is displayed the behaviour of a robot 
located between two stationary agents which are already in their place, showing 
the response of the controllers, on the y-axis, by varying the position of the 
moving robot, visualised on the x-axis. The output control is computed as an 
average over $100$ measures in which the pose of the agent $(x, y, \theta)$ 
differs by a certain epsilon uniformly distributed in the range $[-0.5, 0.5]$, thus 
to avoid the effects of noise that would be obtained on a single measurement and 
unrealistic artefacts in which the sensors are not continuous. Besides, are shown 
the bands which represent plus and minus standard deviation. As expected, the 
output is a high value, positive or negative respectively when the robot is close to 
an obstacle on the left or on the right, or it is close to $0$ when the distance from 
right and left is equal.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.45\textwidth]{contents/images/net-d1/response-varying_init_position-distributed}%
	\caption{Response of \texttt{net-d1} by varying the initial position.}
	\label{fig:net-d1responseposition}
\end{figure}

Finally, in Figure \ref{fig:net-d1distance} is presented another useful metric that 
measures the absolute distance of each robot from the target, visualised on the 
y-axis, over time. This value is averaged on all robots among all the simulation 
runs. The median value is shown as well as the interquartile and interdecile ranges.
On average, the distance from goal of the learned controller is lower than the one 
obtained with the manual controller, meaning that in the final configuration 
the robots moved following the learned controller are closer to the target than 
those moved with the manual one, which are on average at a distance of about 
$1$cm from the goal position. 
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.65\textwidth]{contents/images/net-d1/distances-from-goal-compressed-distributed}%
	\caption[Evaluation of \texttt{net-d1} distances from goal.]{Comparison of 
		performance in terms of distances from goal obtained using three 
		controllers: 
		the expert, the manual and the one learned from \texttt{net-d1}.}
	\label{fig:net-d1distance}
\end{figure}

As mentioned before, in case of \texttt{prox\_values} inputs the 
experiment performed with an \texttt{avg\_gap} of $24$ is not meaningful since 
this value exceeds the maximal range of the sensor. Similarly, since $14$ is the 
maximum range, it is difficult to use this type of input when the \texttt{avg\_gap}  
is $13$, as shown by the losses in Figure \ref{fig:distlossprox_values}.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.85\textwidth]{contents/images/task1/loss-distributed-prox_values@}%
	\caption{Comparison of the losses of the models that use \texttt{prox\_values} 
		readings.}
	\label{fig:distlossprox_values}
\end{figure}

\paragraph*{Results using \texttt{prox\_comm} input}
Following are shown the results of the experiments obtained using the 
\texttt{prox\_comm} readings. 
In Figure \ref{fig:distlossprox_comm}, we analyse the losses by varying the 
average gap. From a first observation, the network seems to be able to work with 
all the gaps.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.85\textwidth]{contents/images/task1/loss-distributed-prox_comm@}%
	\caption{Comparison of the losses of the models that use \texttt{prox\_comm} 
		readings.}
	\label{fig:distlossprox_comm}
\end{figure}

For the assumptions made before, we believe that the model obtained from  
\texttt{net-d6}, the has a higher average gap, is the more promising. 
Moreover, as shown from the \gls{r2} coefficients in Figure \ref{fig:net-d6r2}, we 
expect that the robots’ behaviour using the learned instead of the manual 
controller is better, even if far from the expert.

%Examining  in Figure \ref{fig:net-d456r2}, the higher 
%value is obtained with. 

%\begin{figure}[!htb]
%	\begin{center}
%		\begin{subfigure}[h]{0.49\textwidth}
%			
%\includegraphics[width=\textwidth]{contents/images/net-d4/regression-net-d4-vs-omniscient}%
%		\end{subfigure}
%		\hfill\vspace{-0.5cm}
%		\begin{subfigure}[h]{0.49\textwidth}
%			
%\includegraphics[width=\textwidth]{contents/images/net-d5/regression-net-d5-vs-omniscient}%
%		\end{subfigure}
%	\end{center}
%	\begin{center}
%		\begin{subfigure}[h]{0.49\textwidth}
%			
%\includegraphics[width=\textwidth]{contents/images/net-d6/regression-net-d6-vs-omniscient}
%		\end{subfigure}
%	\end{center}
%	\caption[Comparison of the \gls{r2} coefficients for \texttt{prox\_comm} 
%	readings.]{Comparison of the \gls{r2} coefficients of the models that use 
%		\texttt{prox\_comm} readings.}
%	\label{fig:net-d456r2}
%\end{figure}
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d6/regression-manualvsomniscient}%
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d6/regression-net-d6-vs-omniscient}
	\end{subfigure}
	\caption[Evaluation of the \gls{r2} coefficients of \texttt{net-d6} .]{Comparison 
	of the \gls{r2} coefficient of the manual and the controller learned from 
	\texttt{net-d6} with respect to the omniscient one.}
	\label{fig:net-d6r2}
\end{figure}

In Figure \ref{fig:net-d6traj1}, we show a comparison of the trajectories obtained 
for a sample simulation, using the three controllers.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.7\textwidth]{contents/images/net-d6/animation-distributed1}%
	\caption[Evaluation of the trajectories obtained with \texttt{prox\_comm} 
	input.]{Comparison of trajectories, of a single simulation, generated using three 
		controllers: the expert, the manual and the one learned from \texttt{net-d6}.}
	\label{fig:net-d6traj1}
\end{figure}  
We immediately see that the agents moved using the omniscient controller 
reach the target in less then $15$ time steps. Those moved using the manual 
controller did not approach the goal, even if they try to position themselves at 
equal distances. Instead, the learned controller lets the robots arrive in the 
correct final configuration in $10$ time steps, faster than the expert. This 
because, in this case, the initial positions of the agents moved with the omniscient 
controller are farther than in the other case, so it takes longer to reach the goal.

\bigskip
In Figure \ref{fig:net-d6traj} are shown the trajectories obtained employing the 
three controllers, averaged over all the runs.
As expected, the convergence to the target is slower than before, even for the 
expert, since the distance between the robots is greater, but it is still much faster 
than with the other two controllers.
The manual controller has serious
\begin{figure}[!htb]
	\begin{center}
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=.9\textwidth]{contents/images/net-d6/position-overtime-omniscient}%
			\caption{Expert controller trajectories.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=.9\textwidth]{contents/images/net-d6/position-overtime-learned_distributed}
			\caption{Distributed controller trajectories.}
		\end{subfigure}
	\end{center}
	\caption[Evaluation of the trajectories learned by \texttt{net-d6}.]{Comparison 
	of trajectories, of all the simulation runs, generated using three controllers: the 
	expert, the manual and the one learned from \texttt{net-d6}.}
\end{figure}
\begin{figure}[!htb]\ContinuedFloat
	\begin{center}
	\begin{subfigure}[h]{0.49\textwidth}
		\centering			
		\includegraphics[width=.9\textwidth]{contents/images/net-d6/position-overtime-manual}%
		\caption{Manual controller trajectories.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=.9\textwidth]{contents/images/net-d6/position-overtime-learned_distributed}
		\caption{Distributed controller trajectories.}
	\end{subfigure}
	\end{center}
	\caption[]{Comparison 
	of trajectories, of all the simulation runs, generated using three controllers: the 
	expert, the manual and the one learned from \texttt{net-d6} (cont.).}
	\label{fig:net-d6traj}
\end{figure}

\noindent
problems in reaching the goal: even if the 
agents try to position themselves at equal distances, they tend to increase the 
average gap between them, creating situations in which the last robot in motion 
hits the fixed one. 
Surprisingly, the learned controller allows the agents to converge to the correct 
configuration by taking more time than the expert does.

An immediate examination of the evolution of the control over time, in Figure 
\ref{fig:net-d6control}, highlights the speed of the expert controller, which in all 
the simulation runs, after $25$ time steps, has reached 0. 
In addition, the manual controller always sets a positive speed, which leads to the 
wrong behaviour mentioned earlier, while the slowness of the distributed 
control is explained by the usage of a low speed.
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d6/control-overtime-omniscient}%
		\caption{Expert controller.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d6/control-overtime-manual}%
		\caption{Manual controller.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d6/control-overtime-learned_distributed}
		\caption{Distributed controller.}
	\end{subfigure}
	\caption[Evaluation of the control decided by \texttt{net-d6}.]{Comparison 
		of output control decided using three controllers: the expert, the manual 
		and the one learned from \texttt{net-d6}.}
	\label{fig:net-d6control}
\end{figure}

Figure \ref{fig:net-d6responsesensors} visualises the response of the learned 
controller as the input sensing changes, analysing the same two cases as before. 
Despite the behaviour is the same obtained using \texttt{prox\_values} when the 
robot sees only behind, this time the trend is different when the robot sees 
nothing behind: since the robot has to move backwards, a negative speed is 
always returned, that is higher when the obstacle is far. 
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d6/response-net-d6-front}%
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d6/response-net-d6-rear}
	\end{subfigure}
	\caption{Response of \texttt{net-d6} by varying the input sensing.}
	\label{fig:net-d6responsesensors}
\end{figure}

In Figure \ref{fig:net-d6responseposition}, the behaviour of a robot 
located between other two that are already in their place is displayed.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.45\textwidth]{contents/images/net-d6/response-varying_init_position-distributed}%
	\caption{Response of \texttt{net-d6} by varying the initial position.}
	\label{fig:net-d6responseposition}
\end{figure}
It visualises the response of the learned controller by varying the distance 
between two stationary agents and a robot located among them.
As expected, the output is a high positive value when the robot is close to an 
obstacle on the left, it decreases and reaches $0$ when the distance from 
right and left is equal, and finally becomes negative when there is an obstacle in 
front and not behind. 

\bigskip
Finally, in Figure \ref{fig:net-d6distance}, is presented the average distance of the 
robots from the target among all the simulations. The performance of the learned 
and the manual controllers are similar: in the final configuration they both are at 
about $5$cm from the target. 
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.65\textwidth]{contents/images/net-d6/distances-from-goal-compressed-distributed}%
		\caption[Evaluation of \texttt{net-d6} distances from goal.]{Comparison of 
		performance in terms of distances from goal obtained using three 
		controllers: the expert, the manual and the one learned from \texttt{net-d6}.}
	\label{fig:net-d6distance}
\end{figure}

We conclude the first group of experiments presenting the results obtained 
using both types of input together from which we expect a more stable and 
robust behaviour. 

\paragraph*{Results using \texttt{all\_sensors} input}
In Figure \ref{fig:distlossall}, an analysis of the losses shows that using 
\texttt{all\_sensors} inputs the network is able to work well with all the 
gaps. 
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.8\textwidth]{contents/images/task1/loss-distributed-all_sensors@}%
	\caption{Comparison of the losses of the models that use \texttt{all\_sensors} 
		readings.}
	\label{fig:distlossall}
\end{figure}

Examining the \gls{r2} coefficients in Figure \ref{fig:net-d789r2}, the behaviour 
obtained with \texttt{net-d7} and \texttt{net-d9} are the more promising. 
\begin{figure}[!htb]
	\begin{center}
		\begin{subfigure}[h]{0.49\textwidth}
 			\includegraphics[width=\textwidth]{contents/images/net-d7/regression-net-d7-vs-omniscient}%
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.49\textwidth}
			\includegraphics[width=\textwidth]{contents/images/net-d8/regression-net-d8-vs-omniscient}%
		\end{subfigure}
	\end{center}
	\hfil\vspace{-0.8cm}
	\begin{center}
		\begin{subfigure}[h]{0.49\textwidth}
			\includegraphics[width=\textwidth]{contents/images/net-d9/regression-net-d9-vs-omniscient}
		\end{subfigure}
	\end{center}
	\caption[Comparison of the \gls{r2} coefficients for \texttt{prox\_comm} 
	readings.]{Comparison of the \gls{r2} coefficients of the models that use 
		\texttt{prox\_comm} readings.}
	\label{fig:net-d789r2}
\end{figure}

\bigskip
Considering the more complex case, that is the one with the greatest average gap,
the superiority of this controller is further supported by the comparisons in Figure
\ref{fig:net-d9r2}. 
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d9/regression-manualvsomniscient}%
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d9/regression-net-d9-vs-omniscient}
	\end{subfigure}
	\caption[Evaluation of the \gls{r2} coefficients of \texttt{net-d9}.]{Comparison 
		of the \gls{r2} coefficient of the manual and the controller learned from 
		\texttt{net-d9} with respect to the omniscient one.}
	\label{fig:net-d9r2}
\end{figure}

In Figure \ref{fig:net-d9traj1} is shown a comparison of the trajectories for a 
sample simulation.
As before, the performance obtained using the omniscient and the learned 
controllers are comparable: both are very fast and in less than 10 time steps they 
reach the target. 
Instead, when the agents moved using the manual controller have almost 
approached the target, they start to oscillate. This problem is not surprising, since 
the parameters of the manual controller have been tuned on some specific cases 
and are always the same for all datasets, sometimes it does not work correctly.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.65\textwidth]{contents/images/net-d9/animation-distributed}%
	\caption[Evaluation of the trajectories obtained with \texttt{all\_sensor} 
	input.]{Comparison of trajectories, of a single simulation, generated using three 
		controllers: the expert, the manual and the one learned from \texttt{net-d9}.}
	\label{fig:net-d9traj1}
\end{figure}  

\begin{figure}[!htb]
	\begin{center}
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=.9\textwidth]{contents/images/net-d9/position-overtime-omniscient}%
			\caption{Expert controller trajectories.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=.9\textwidth]{contents/images/net-d9/position-overtime-learned_distributed}
			\caption{Distributed controller trajectories.}
		\end{subfigure}
	\end{center}
\vspace{-0.5cm}
	\caption[Evaluation of the trajectories learned by 
\texttt{net-d9}.]{Comparison of trajectories, of all the simulation runs, 
	generated using three controllers: the expert, the manual and the one learned 
	from \texttt{net-d9}.}
\end{figure}
\begin{figure}[!htb]\ContinuedFloat
	\begin{center}
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=.9\textwidth]{contents/images/net-d9/position-overtime-manual}%
			\caption{Manual controller trajectories.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=.9\textwidth]{contents/images/net-d9/position-overtime-learned_distributed}
			\caption{Distributed controller trajectories.}
		\end{subfigure}
	\end{center}
	\caption[]{Comparison of trajectories, of all the simulation runs, 
	generated using three controllers: the expert, the manual and the one learned 
	from \texttt{net-d9} (cont.).}
	\label{fig:net-d9traj}
\end{figure}

In Figure \ref{fig:net-d9traj} are shown trajectories obtained employing the three 
controllers. The convergence to the target is still slow, even if this time the expert 
needs less time steps than before. 
The manual controller does not show the same problem as before, while the 
learned controller is still the slowest to end up in the correct configuration.

Examining the evolution of the output control, in Figure \ref{fig:net-d9control}, 
the plots of the expert and the learned controllers are similar, although the speed 
in the second is much lower.
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d9/control-overtime-omniscient}%
		\caption{Expert controller.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d9/control-overtime-manual}%
		\caption{Manual controller.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d9/control-overtime-learned_distributed}
		\caption{Distributed controller.}
	\end{subfigure}
	\caption[Evaluation of the control decided by \texttt{net-d9}.]{Comparison 
		of output control decided using three controllers: the expert, the manual 
		and the one learned from \texttt{net-d9}.}
	\label{fig:net-d9control}
\end{figure}

In Figure \ref{fig:net-d9responseposition} is displayed the behaviour of a robot 
located between other two that are already in their place.
This time the trend of the three curves shows how the behaviour of the model 
learned and of the manual controller are similar to that of the expert.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.45\textwidth]{contents/images/net-d9/response-varying_init_position-distributed}%
	\caption{Response of \texttt{net-d9} by varying the initial position.}
	\label{fig:net-d9responseposition}
\end{figure}

Finally, in Figure \ref{fig:net-d9distance}, is presented the absolute distance of 
each robot from the target, averaged on all robots among all the simulation runs. 
The median value is shown as well as the interquartile and interdecile ranges.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.63\textwidth]{contents/images/net-d9/distances-from-goal-compressed-distributed}%
	\caption[Evaluation of \texttt{net-d9} distances from goal.]{Comparison of 
		performance in terms of distances from goal obtained using three 
		controllers: the expert, the manual and the one learned from \texttt{net-d9}.}
	\label{fig:net-d9distance}
\end{figure}
As anticipated by the trajectories in Figure \ref{fig:net-d9traj}, the controller 
learned from \texttt{net-d9} is slower to converge than the manual one. In fact, 
this plot confirms that the agents moved following a manual controller in the final 
configuration are closer to the target than those moved by the distributed 
controller: they are on average $2$ or $7$cm away from the goal position 
respectively.

\paragraph*{Summary}
We show in the figures below the losses of the trained models as the different 
inputs of the network vary, in particular, we represent with the blue, the orange 
and the green lines \texttt{prox\_values}, \texttt{prox\_comm} and
\texttt{all\_sensors} inputs respectively.
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/task1/loss-distributed-gap_8@copy}%
		\caption{\texttt{avg\_gap} of $8$cm.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/task1/loss-distributed-gap_13@copy}%
		\caption{\texttt{avg\_gap} of $13$cm.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/task1/loss-distributed-gap_24@copy}
		\caption{\texttt{avg\_gap} of $24$cm.}
	\end{subfigure}
	\caption[Losses summary of the first set of experiments.]{Comparison 
		of the losses by varying the input of the networks for the three gaps.}
	\label{fig:distloss81324}
\end{figure}

In case of an \texttt{avg\_gap} of $8$cm, the model trained using 
\texttt{prox\_values} has a lower loss, following is the network that employ 
\texttt{all\_sensors}, with similar results, and finally the model that works with 
\texttt{prox\_comm}.
This performance is expected, as well as the fact that \texttt{all\_sensors} cannot 
perform better than \texttt{prox\_values} with small gaps, since in this case the 
data coming from \texttt{prox\_comm} contains only zeros in the the second half 
of the array, making it unusable.
In a complementary way, by increasing the gap to $13$cm,  
\texttt{prox\_values} alone is not able to achieve satisfactory results, while used 
together with \texttt{prox\_comm}, \texttt{all\_sensors} reaches good 
performances that on the validation set are comparable to those obtained using 
\texttt{prox\_comm} alone.
Finally, by increasing the gap even more, up to $24$cm, 
\texttt{prox\_values} becomes completely unusable, while \texttt{prox\_comm} 
and \texttt{all\_sensors} still have excellent performances similar to each other.


\subsubsection{Experiment 2: variable number of agents}
\label{subsubsec:task1-exp-distr-2}
The second group of experiments we carried out using a distributed approach, 
examines the behaviour of the control learned using \texttt{all\_sensors} inputs. 
\begin{figure}[H]
	\centering
	\begin{tabular}{ccccc}
		\toprule
		\textbf{Model} \quad & \textbf{\texttt{network\_input}} & 
		\textbf{\texttt{input\_size}} & \textbf{\texttt{avg\_gap}} & \textbf{\texttt{N}}\\
		\midrule
		\texttt{net-d10} 	& \texttt{all\_sensors}		&  $14$  &  $8$		 	 &	$5$ \\
		\texttt{net-d11} 	& \texttt{all\_sensors}		&  $14$  &  $20$		&	$5$ \\
		\texttt{net-d12} 	& \texttt{all\_sensors}		&  $14$  &  variable   &	$5$ \\
		\texttt{net-d13} 	& \texttt{all\_sensors}	  	&  $14$  &  $8$			 &	  $8$ \\
		\texttt{net-d14} 	& \texttt{all\_sensors}	  	&  $14$  &  $20$   		&	 $8$ \\
		\texttt{net-d15} 	& \texttt{all\_sensors}	  	&  $14$  &  variable	&	 $8$ \\
		\texttt{net-d16} 	& \texttt{all\_sensors}	  	&  $14$  &  $ 8$		  &	 variable\\
		\texttt{net-d17} 	& \texttt{all\_sensors}	  	&  $14$  &  $20$		 &	variable\\
		\texttt{net-d18} 	& \texttt{all\_sensors}	  	&  $14$  &  variable	 &	
		variable\\
		\bottomrule
	\end{tabular}
	\captionof{table}[Experiments with variable agents and gaps (no 
	communication).]{List of the experiments carried out using a variable 
		number of agents and of gaps.}
	\label{tab:modeldist}
\end{figure}
In this situation, the simulation runs use a different number of robots $N$, that 
can be fixed at $5$ or $8$ for the entire simulation, or even vary in the range $[5, 
10]$. The same reasoning is applied to the choice of the \texttt{avg\_gap}, that 
can be a fixed value in all the runs, chosen between $8$ or $20$, but also vary in 
the range $[5, 24]$. 
The objective of this set of experiments, summarised in Table \ref{tab:modeldist}, 
is to verify the robustness of the models, proving that it is possible to train 
networks that handle a variable number of agents. 

First of all, we start by showing in Figure \ref{fig:distlossext} an overview of the 
models performance in terms of train and validation losses. 
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.8\textwidth]{contents/images/task1-extension/loss-distributed-all@}%
	\caption[Comparison of losses of the second set of 
	experiments.]{Comparison of the losses of the models carried out using a 
		variable number of agents and of average gap.}
	\label{fig:distlossext}
\end{figure}

\paragraph*{Results using 5 agents}
In Figure \ref{fig:distlossn5} are analysed the experiments performed using a 
fixed number of agents, the same used for the group of experiments presented 
above, i.e., $5$, in order to show the difference of performance using a gap that 
is first small, then large and finally variable.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=.8\textwidth]{contents/images/task1-extension/loss-distributed-N5@}%
	\caption[Comparison of the losses of the models that use $5$ 
	agents.]{Comparison of the losses of the models that use $5$ agents as 
	the gap varies.}
	\label{fig:distlossn5}
\end{figure}

Examining more in detail the case in which the model is trained using a 
variable average gap, in Figure \ref{fig:net-d12r2} is visualised a comparison of 
the \gls{r2} of the manual and the learned controllers, on the validation set. 
The robots' behaviour using the learned instead of the manual controller is a 
bit better, even if far from the expert.
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d12/regression-manualvsomniscient}%
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d12/regression-net-d12-vs-omniscient}
	\end{subfigure}
	\caption[Evaluation of the \gls{r2} coefficients of \texttt{net-d12} 
	.]{Comparison 
		of the \gls{r2} coefficients of the manual and the controller learned from 
		\texttt{net-d12} with respect to the omniscient one.}
	\label{fig:net-d12r2}
\end{figure}

In Figure \ref{fig:net-d12traj1}, we first show a sample simulation: on the y-axis 
are visualised the positions of the agents, while on the x-axis the simulation time 
steps. 
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.7\textwidth]{contents/images/net-d12/animation-distributed}%
	\caption[Evaluation of the trajectories obtained with 5 agents.]{Comparison of 
	trajectories, of a single simulation, generated using three 
		controllers: the expert, the manual and the one learned from 
		\texttt{net-d12}.}
	\label{fig:net-d12traj1}
\end{figure}
The agents moved using the omniscient controller are, as expected, those that 
reach the target faster. Those moved using the manual controller are slower and 
moreover, as for the case shown in Figure \ref{fig:net-d9traj1}, they start to 
oscillate when they approach the target. Instead, the learned controller, even if is 
slower than the other two, is able to reach the correct configuration. 

In Figure \ref{fig:net-d12traj}, we show first a comparison of the expert and the 
learned trajectories, and then between the manual and the learned ones. 
In particular, on the y-axis is visualised the position of each agent over time, 
averaged over all the simulation runs, while on the x-axis the simulation 
time steps. It is important to note that there is a difference in these graphs 
compared to those of the previous group of experiments. 
\begin{figure}[!htb]
	\begin{center}
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=.95\textwidth]{contents/images/net-d12/position-overtime-omniscient}%
			\caption{Expert controller trajectories.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{contents/images/net-d12/position-overtime-learned_distributed}
			\caption{Distributed controller trajectories.}
		\end{subfigure}
	\end{center}
	\begin{center}
		\begin{subfigure}[h]{0.49\textwidth}
			\centering			
			\includegraphics[width=.95\textwidth]{contents/images/net-d12/position-overtime-manual}%
			\caption{Manual controller trajectories.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{contents/images/net-d12/position-overtime-learned_distributed}
			\caption{Distributed controller trajectories.}
		\end{subfigure}
	\end{center}
	\vspace{-0.5cm}
	\caption[Evaluation of the trajectories learned by 
	\texttt{net-d12}.]{Comparison of trajectories, of all the simulation runs, 
	generated using three controllers: the expert, the manual and the one learned 
	from \texttt{net-d12}.}
	\label{fig:net-d12traj}
\end{figure}

\noindent
Observing the deviation of the position of the robots with respect to the average, 
the last agent of the row did not maintain the same initial and goal positions 
throughout the simulations since the average gap set is different for every run.
The convergence of the robots to the target is guaranteed in 20 time steps using 
the expert, while the manual and learned controller still manage to reach the 
correct configuration with more time.

Analysing the evolution of the control over time, in Figure 
\ref{fig:net-d12control}, we observe that the speeds set by the manual controller 
and the one learned from the network are significantly lower and therefore do 
not allow to reach the target in a satisfactory time.
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d12/control-overtime-omniscient}%
		\caption{Expert controller.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d12/control-overtime-manual}%
		\caption{Manual controller.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d12/control-overtime-learned_distributed}
		\caption{Distributed controller.}
	\end{subfigure}
	\caption[Evaluation of the control decided by \texttt{net-d12}.]{Comparison of 
	output control decided using three controllers: the expert, the manual and the 
	one learned from \texttt{net-d12}.}
	\label{fig:net-d12control}
\end{figure}

In Figure \ref{fig:net-d12responseposition}, another informative plot displays 
the behaviour of a robot located between other two that are already in their place.
As expected, the trend of the three curves shows how the behaviour of the 
model learned and of the manual controller are similar.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.45\textwidth]{contents/images/net-d12/response-varying_init_position-distributed}%
	\caption{Response of \texttt{net-d12} by varying the initial position.}
	\label{fig:net-d12responseposition}
\end{figure}
However, the performance of the manual controller, when the robot is close to 
the one that preceded it, is practically comparable to that obtained by the 
expert.
In contrast, when the agent is close to the following one, it worsens and 
presents a lot of variability.

Finally, in Figure \ref{fig:net-d12distance}, is shown the absolute distance of 
each robot from the target, averaged on all robots among all the simulation runs, 
over time. 
Despite we expected performances similar to those presented in the Figure 
\ref{fig:net-d9distance}, in this circumstance the agents moved following the 
distributed controller are closer to the target, in particular in the final 
configuration they are on average $2$cm away from the goal position. 
Furthermore, it is shown that there are far fewer cases away from the average 
behaviour.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.65\textwidth]{contents/images/net-d12/distances-from-goal-compressed-distributed}%
	\caption[Evaluation of \texttt{net-d1} distances from goal.]{Comparison of 
		performance in terms of distances from goal obtained using three 
		controllers: 
		the expert, the manual and the one learned from \texttt{net-d12}.}
	\label{fig:net-d12distance}
\end{figure}

\paragraph*{Results using 8 agents}
Following are shown the losses of the models trained using an higher number of 
agents, i.e.,8, by varying the average gap. From a first observation, 
\begin{figure}[H]
	\centering
	\includegraphics[width=.75\textwidth]{contents/images/task1-extension/loss-distributed-n8@}%
	\caption[Comparison of the losses of the models that use $8$ 
	agents.]{Comparison of the losses of the models that use $8$ agents as the gap 
	varies.}
	\label{fig:distlossn8}
\end{figure}

\noindent
the network seems to be able to work with all the gaps.
As before, for the network it is easier to perform a task using a smaller gap.
For this reason, it is more interesting to analyse the case in which the model 
is trained using a variable gap.

In Figure \ref{fig:net-d15r2} is visualised a comparison of the \gls{r2} 
coefficients of the manual and the learned controller. In both cases, the 
coefficients are very low, since in most of the cases in which the controllers have 
to decide a zero or maximum speed a wrong value is predicted, however, the 
coefficient obtained from the network is slightly better.
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d15/regression-manualvsomniscient}%
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d15/regression-net-d15-vs-omniscient}
	\end{subfigure}
	\caption[Evaluation of the \gls{r2} coefficients of \texttt{net-d15} 
	.]{Comparison of the \gls{r2} coefficients of the manual and the controller 
		learned from \texttt{net-d15} with respect to the omniscient one.}
	\label{fig:net-d15r2}
\end{figure}

In Figure \ref{fig:net-d15traj1} is displayed a sample simulation that shows on 
the y-axis position of  
\begin{figure}[H]
	\centering
	\includegraphics[width=.7\textwidth]{contents/images/net-d15/animation-distributed}%
	\caption[Evaluation of the trajectories obtained with 8 agents.]{Comparison of 
	trajectories, of a single simulation, generated using three controllers: the 
	expert, the manual and the one learned from \texttt{net-d15}.}
	\label{fig:net-d15traj1}
\end{figure}

\noindent
each agent, while on the x-axis the simulation time steps.
The agents moved using the learned controller, even if in the initial configuration 
are far from the target, they are able to reach the goal. 
Instead, the agents moved using the manual controller, when have almost 
approached the target, they start to oscillate. 

In Figure \ref{fig:net-d15traj} are shown the trajectories obtained employing 
the three controllers, averaged over all the simulation runs.
Compared to the previous case, a greater number of robots implies a 
slowdown in reaching the correct position, even when using an expert 
controller.
As before, the convergence of the robots using the manual and learned 
controllers needs more time.
\begin{figure}[!htb]
	\begin{center}
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=.9\textwidth]{contents/images/net-d15/position-overtime-omniscient}%
			\caption{Expert controller trajectories.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=.9\textwidth]{contents/images/net-d15/position-overtime-learned_distributed}
			\caption{Distributed controller trajectories.}
		\end{subfigure}
	\end{center}
	\begin{center}
		\begin{subfigure}[h]{0.49\textwidth}
			\centering			
			\includegraphics[width=.9\textwidth]{contents/images/net-d15/position-overtime-manual}%
			\caption{Manual controller trajectories.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.49\textwidth}
			\centering
			\includegraphics[width=.9\textwidth]{contents/images/net-d15/position-overtime-learned_distributed}
			\caption{Distributed controller trajectories.}
		\end{subfigure}
	\end{center}
	\caption[Evaluation of the trajectories learned by 
	\texttt{net-d15}.]{Comparison of trajectories, of all the simulation runs, 
	generated using three controllers: the expert, the manual and the one learned 
	from \texttt{net-d15}.}
	\label{fig:net-d15traj}
\end{figure}

Examining in Figure \ref{fig:net-d15control} the evolution of the control over 
time, the graph of the distributed controller highlights how the speed decided by 
the model has further decreased due to the increase in the amount of agents in 
the simulation.
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d15/control-overtime-omniscient}%
		\caption{Expert controller.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d15/control-overtime-manual}%
		\caption{Manual controller.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d15/control-overtime-learned_distributed}
		\caption{Distributed controller.}
	\end{subfigure}
	\caption[Evaluation of the control decided by \texttt{net-d15}.]{Comparison 
		of output control decided using three controllers: the expert, the manual 
		and the one learned from \texttt{net-d15}.}
	\label{fig:net-d15control}
\end{figure}

Figure \ref{fig:net-d15responseposition} displays the behaviour of a robot 
located between other two that are already in their place.
Analysing the way of acting of the three controllers for this experiment, from 
the plot arises an important difference in the decisions taken by the distributed 
and the manual controllers.
The learned controller, whether a robot is closer to the one that precedes it 
or to the one following it, sets a proportional speed, lower than the optimal one, 
that leads it to move respectively back and forth to reach the desired position.
Instead, the manual controller when an agent is closer to the one in front 
sets a very high speed to move quickly to the desired position, just like the expert 
does, unlike when the robot is closer to the one following it, where it sets a 
negative speed but not high enough, a bit like the distributed controller does.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.45\textwidth]{contents/images/net-d15/response-varying_init_position-distributed}%
	\caption{Response of \texttt{net-d15} by varying the initial position.}
	\label{fig:net-d15responseposition}
\end{figure}

\bigskip
Finally, in Figure \ref{fig:net-d15distance} is presented the average distance of 
the robots from the target among all the simulations. The performance of the 
learned and the manual controllers are different from before: \texttt{net-d15} is 
slower to converge. In fact, this plot 
confirms that the agents moved following a manual controller in the final 
configuration are closer to the target than those moved by the distributed 
controller, respectively, they are on average $2$ or $3.5$cm away from the 
goal position. Moreover, observing the coloured bands we see that there is a lot of 
variance in the distributed controller final positions, in fact there are runs in which 
some agents can be even $10$cm far from the target.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.65\textwidth]{contents/images/net-d15/distances-from-goal-compressed-distributed}%
	\caption[Evaluation of \texttt{net-d15} distances from goal.]{Comparison 
	of performance in terms of distances from goal obtained using three 
	controllers: the expert, the manual and the one learned from \texttt{net-d15}.}
	\label{fig:net-d15distance}
\end{figure}

\paragraph*{Results using variable agents}
We conclude the experiments performed using a distributed approach by 
presenting the results obtained with a variable number of agents. 
\begin{figure}[H]
	\centering
	\includegraphics[width=.8\textwidth]{contents/images/task1-extension/loss-distributed-Nvar@}%
	\caption[Comparison of the losses of the models that use variable 
	agents.]{Comparison of the losses of the models that use variable agents 
	and gaps.}
	\label{fig:distlossnvar}
\end{figure}

\noindent
In Figure \ref{fig:distlossnvar}, are analysed the losses by varying the average 
gap. As before, for the network it is easier to perform a task using a smaller gap 
and in general training the model on a variable gap performs better than on a 
fixed but big gap.

Dwelling on the most interesting case, the one in with both average gap and 
number of agents variable, the \gls{r2} coefficients shown in Figure 
\ref{fig:net-d18r2} are still very low.
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d18/regression-manualvsomniscient}%
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d18/regression-net-d18-vs-omniscient}
	\end{subfigure}
	\caption[Evaluation of the \gls{r2} coefficients of \texttt{net-d18} 
	.]{Comparison of the \gls{r2} coefficient of the manual and the controller 
	learned from \texttt{net-d18} with respect to the omniscient one.}
	\label{fig:net-d18r2}
\end{figure}

In Figure \ref{fig:net-d9traj1} is shown a comparison of the trajectories obtained 
for a sample simulation.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.65\textwidth]{contents/images/net-d18/animation-distributed}%
	\caption[Evaluation of the trajectories obtained with variable 
	agents.]{Comparison of trajectories, of a single simulation, generated using 
		three controllers: the expert, the manual and the one learned from 
		\texttt{net-d18}.}
	\label{fig:net-d18traj1}
\end{figure}  
In this example, in the simulation there are 10 agents that are always able to 
reach the target when moved using an omniscient controller.
When they use the manual controller, the same oscillation issue occurs in 
proximity  to the goal. Instead, the learned controller is certainly the slowest, and 
after 38 time steps not all robots are in the correct position.

Since the number of agents is variable, we show different plots, depending on this 
quantity, for the trajectories obtained employing the three controllers: we analyse, 
in the following figures, cases with 5, 8 and 10 agents. 
\begin{figure}[!htb]
	\begin{center}
		\begin{subfigure}[h]{0.325\textwidth}
			\centering
			\includegraphics[width=\textwidth]{contents/images/net-d18/N5/position-overtime-omniscient}%
			\caption{Expert controller.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.325\textwidth}
			\centering
			\includegraphics[width=\textwidth]{contents/images/net-d18/N5/position-overtime-manual}%
			\caption{Manual controller.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.325\textwidth}
			\centering
			\includegraphics[width=\textwidth]{contents/images/net-d18/N5/position-overtime-distributed}
			\caption{Distributed controller.}
		\end{subfigure}
	\end{center}
	\begin{center}
		\begin{subfigure}[h]{0.325\textwidth}
			\centering
			\includegraphics[width=\textwidth]{contents/images/net-d18/N8/position-overtime-omniscient}%
			\caption{Expert controller.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.325\textwidth}
			\centering
			\includegraphics[width=\textwidth]{contents/images/net-d18/N8/position-overtime-manual}%
			\caption{Manual controller.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[h]{0.325\textwidth}
			\centering
			\includegraphics[width=\textwidth]{contents/images/net-d18/N8/position-overtime-distributed}
			\caption{Distributed controller.}
		\end{subfigure}
	\end{center}
	\begin{center}
		\begin{subfigure}[h]{0.325\textwidth}
			\centering
			\includegraphics[width=\textwidth]{contents/images/net-d18/N10/position-overtime-omniscient}%
			\caption{Expert controller.}
		\end{subfigure}
		\hfill
	\begin{subfigure}[h]{0.325\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d18/N10/position-overtime-manual}%
		\caption{Manual controller.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.325\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d18/N10/position-overtime-distributed}
		\caption{Distributed controller.}
	\end{subfigure}
\end{center}
\vspace{-0.5cm}
	\caption[Trajectories learned by \texttt{net-d18} using 5, 8 and 10 
	agents.]{Comparison of trajectories, of all the simulation runs, of 5, 8 and 10 
	agents generated using three controllers.}
	\label{fig:net-d18traj10}
\end{figure}
From a first observation it is confirmed that increasing the number of robots in 
the simulation implies a greater number of time steps to reach the final 
configuration.
Furthermore, with a large number of agents it is common for biases to add up 
and for the error to become more significant, in particular the one of the central 
robot of the group.
The convergence is still slow, even if this time the expert need less time steps than 
before. 
The learned controller is still the slowest to end up in the correct configuration.

Examining the evolution of the output control, in Figure \ref{fig:net-d18control}, 
the graph of the distributed controller highlights how the speed decided by the 
model has decreased due to the increase in the amount of agents in the 
simulation.
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d18/control-overtime-omniscient}%
		\caption{Expert controller.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d18/control-overtime-manual}%
		\caption{Manual controller.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/net-d18/control-overtime-learned_distributed}
		\caption{Distributed controller.}
	\end{subfigure}
	\caption[Evaluation of the control decided by \texttt{net-d18}.]{Comparison 
		of output control decided using three controllers: the expert, the manual and 
		the one learned from \texttt{net-d18}.}
	\label{fig:net-d18control}
\end{figure}


In Figure \ref{fig:net-d18responseposition} is displayed the behaviour of a robot 
located between other two that are already in their place.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.45\textwidth]{contents/images/net-d18/response-varying_init_position-distributed}%
	\caption{Response of \texttt{net-d18} by varying the initial position.}
	\label{fig:net-d18responseposition}
\end{figure}
In this case the same reasoning made for Figure 
\ref{fig:net-d15responseposition} applies.

Focusing on the absolute distance of each robot from the target, presented in 
Figure \ref{fig:net-d18distance}, we observe once again that the agents moved 
following a manual controller in the final configuration are closer to the target 
than those moved with the learned one.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.65\textwidth]{contents/images/net-d18/distances-from-goal-compressed-distributed}%
	\caption[Evaluation of \texttt{net-d18} distances from goal.]{Comparison of 
		performance in terms of distances from goal obtained using three 
		controllers: the expert, the manual and the one learned from 
		\texttt{net-d18}.}
	\label{fig:net-d18distance}
\end{figure}

\paragraph*{Summary}
To summarise the performance, as the number of agents vary for each gap, we 
show once again in the figures below the losses of the trained models.
In case of an \texttt{avg\_gap} of $8$cm, the model trained using 
a minor number of agents, as expected has a lower loss, following, with very 
similar values the model that employs 8 robots and that with a variable number of 
agents.
Finally, by choosing a variable gap and number of agents the performance are 
better than those generated with fixed but high number of robots. While again, 
the results obtained using fewer agents are the best.
\begin{figure}[!htb]
	\centering
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/task1-extension/loss-distributed-gap_8@copy}%
		\caption{\texttt{avg\_gap} of $8$cm.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/task1-extension/loss-distributed-gap_20@copy}%
		\caption{\texttt{avg\_gap} of $20$cm.}
	\end{subfigure}
	\hfill
	\begin{subfigure}[h]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{contents/images/task1-extension/loss-distributed-gap_var@copy}
		\caption{\texttt{avg\_gap} variable.}
	\end{subfigure}
		\caption[Losses summary of the second set of 
		experiments.]{Comparison of the losses by varying the input of the networks 
		for different gaps.}
	\label{fig:distloss820var}
	\vspace{-0.5cm}
\end{figure}

\subsubsection{Experiment 3: increasing number of agents}
\label{subsubsec:task1-exp-distr-3}
Multi-agent systems present interesting scalability challenges. For this reason, the 
objective of the last group of experiments is to show the behaviour of a network 
trained using \texttt{all\_sensors} input, variable gaps and number of agents, 
applied on simulations with a higher number of robots, from 5 up to 50.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=.5\textwidth]{contents/images/distances-from-goal-compressed-distributed}%
	\caption[Evaluation of distances from goal for a high number of 
	robots.]{Comparison of performance in terms of distances from goal obtained 
	on simulations with an increasing number of robots.}
	\label{fig:distdistr}
\end{figure}
In Figure \ref{fig:distdistr} is visualised, for 5 different experiments, the absolute 
distance of each robot from the target over time. This value is averaged on all 
robots among all the simulation runs. 
In general, as expected, the complexity grows rapidly as the number of agents 
increases and it is common for biases to add up and for the error to become more 
significant.
In the experiment performed with a variable number of agents, i.e.,in the range 
$[5, 10]$, in the final configuration the robots are on average at about $3$cm 
from the goal position. Increasing the number of agents, first to 20, then 30, 40 
and finally 50, the robots are more and more distant, in the worst case $13$cm 
from the target.


\subsubsection{Remarks}
\label{subsubsec:remarks-task1-dist}
In this section, we have shown that using a distributed controller learned by 
imitating an expert it is possible to obtain results more or less comparable to 
those reached employing a manual controller.
The problem presents interesting scalability challenges, and in general, a greater 
number of robots implies a slowdown in reaching the correct positions.
However, this approach is not enough to achieve satisfactory performance. In the 
following section we are going to describe a second approach that solve the 
problem by exploiting a communication protocol between agents.