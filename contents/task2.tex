\section{Task 2: Colouring the robots in space}
\label{sec:task2}

\subsection{Overview}
\label{subsec:desc2}

The second scenario tackles another multi-agent coordination task, colouring the 
robots according to a certain criterion in space. 
The environment, as well as the assumptions, are the same described in Chapter 
\ref{chap:experiments} and in Section \ref{subsec:desc1}.
%FIXME

The only difference lies in the goal to be achieved by the robots: assuming that 
the agents are divided into groups, their purpose is to colour themselves, by 
turning on their top \gls{rgb} \gls{led}, depending on their group membership.

Initially, for the sake of simplicity, we decided to divide the robots into two 
groups. In particular, in case of an even number of agents, the robots in the first 
half of the row belong to the first group and the remaining to the second, while in 
the case of an odd number robots, the agent located in the central position is 
assigned to the first set.

As for the previous task, the problem can be solved performing imitation 
learning, but the role of communication is fundamental. In fact, what makes the 
difference are not the distances perceived by the robot sensors but the messages 
exchanged between the agents, which are they only mean to determine their 
order. 

In this scenario, the two ``dead'' robots play an important role: they always 
communicate a message that indicates that they are the only two agents that 
receive communication just from one side.

Each agent can still update its state by performing actions — in this case not 
moving along the x-axis but colouring the top \gls{led} in red or blue — based on 
the observations received from the environment — the messages received from 
neighbours. 

Then, by following the example of an expert, we train an end-to-end \gls{nn} 
that takes as input the communication — transmitted by the nearest neighbours in 
the previous timestep — and produces the colour of the robot together with the 
message to be sent.

\subsection{Controllers}
\label{subsec:task2controllers}

\subsubsection{Expert controller}
\label{subsubsec:omniscient2}

As disclosed in Section \ref{subsec:controllersmodel}, the first element involved in 
an imitation learning problem is the omniscient controller, also called expert, 
which is able to perceive the environment and the observations of all the agents, 
obtaining a global knowledge of the state of the system. In this way it can use all 
the information to decide the best action to perform for each agent.



\subsubsection{Manual controller}
\label{subsubsec:manual2}

The second controller we implemented is the manual one, whose main 
purpose is to draw conclusions about the quality of the learned controller.

The main difference between this and the previous controller is that, the manual 
one can be considered a local controller, that only has a partial knowledge of the 
environment since it knows only the state of the current agent and its 
observations.

For this scenario,

\subsection{Communication approach}
\label{subsec:task2comm}


\input{contents/task2_experiments}
