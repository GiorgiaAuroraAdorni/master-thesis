\setcounter{chapter}{0}
\chapter{Literature review}
\label{chap:stateoftheart}
This chapter discusses previous research about the topic, providing a brief 
introduction to the approach we adopted.
%, evaluating, in particular, their shortcomings. 

\glsreset{ai}
\glsreset{dl}
\glsreset{rl} 
\glsreset{dnn}

\bigskip
In recent years, the application of \gls{ai} and \gls{dl} techniques to multi-agents 
cooperative problems has become increasingly successful.
Gasser and Huhns, in \emph{Distributed Artificial Intelligence} 
\cite[][]{gasser2014distributed}, have addressed the issue of coordination and 
cooperation among agents with the combination of distributed systems and 
\gls{ai}, a discipline known as \gls{dai}. 

Similarly, Panait and Luke, in their work \emph{Cooperative multi-agent learning: 
The state of the art} \cite[][]{panait2005cooperative}, report that \gls{mal}, the 
application of machine learning to problems involving multiple agents, has 
become a popular approach which deals with unusually large search spaces. 
In addition, they mention three of the most classic techniques used to solve 
cooperative \gls{ma} problems, that are Supervised and Unsupervised Learning, 
and \gls{rl}. These methods are distinguished according to the type of feedback 
provided to the agent: the target output in the first case, no feedback is provided 
in the second, and reward based on the learned output in the last one.

Given these three options, the vast majority of articles in this field used 
reward-based methods to approach \gls{mal} scenarios, in particular \gls{rl}, that 
make they possible to achieve sophisticated goals in complex and uncertain 
environments \cite[][]{oliehoek2012decentralised}. 
However, this technique is notoriously known for being hard, in particular it is 
difficult to design a suitable reward function for the agents to optimise, which 
precisely leads to the desired behaviour in all possible scenarios. This problem is 
further exacerbated in multi-agent settings \cite[][]{hadfield2017inverse, 
oliehoek2012decentralised}.

\gls{irl} addresses this problem by using a given set of expert trajectories to 
derive a reward function under which these are optimal. 
Nevertheless, this technique imposes often unrealistic requirements 
\cite[][]{vsovsic2016inverse}.

\gls{il} is a class of methods that has been successfully applied to a wide range of 
domains in robotics, for example, autonomous driving 
\cite[][]{schaal1999imitation, stepputtis2019imitation}. They aim to overcome the 
issues aforementioned and, unlike reward-based methods, the model 
acquires skills and provides actions to the agents by observing the desired 
behaviour, performed by an expert \cite[][]{song2018multi, zhang2018deep, 
billard2008survey}.
Using this approach the models learn how to extract relevant information from 
the data provided to them, directly learning a mapping from observations to 
actions. 
A more challenging situation occurs when the model also has to infer the 
coordination among agents that is implicit in the demonstrations, using 
unsupervised approaches to imitation.

In this direction, literature suggests that cooperative tasks sometimes cannot be 
solved using only a simple distributed approach, instead it may be necessary to 
allow an explicit exchange of messages between the agents.
In \emph{Multi-agent reinforcement learning: Independent vs. cooperative 
agents} \cite[][]{tan1993multi}, Tan affirms that cooperating learners should use 
communication in a variety of ways in order to improve team performance: they 
can share instantaneous informations as well as episodic experience and learned 
knowledge.
Also Pesce and Montana propose the use of inter-agent communication for 
situations in which the agents can only acquire partial observations and are faced 
with a task requiring coordination and synchronisation skills 
\cite[][]{pesce2019improving}. Their solution consists in an explicit 
communication system that allows agents to exchange messages that are used 
together with local observations to decide which actions to take.

Our work is based on \emph{Learning distributed controllers by backpropagation}
\cite[][]{marcoverna2020}, which proposes an approach in which a distributed 
policy for the agents and a coordination model are learned at the same time. 
This method is based on an important concept introduced in \emph{Coordinated 
multi-agent imitation learning} \cite[][]{le2017coordinated}: the network has the 
ability to autonomously determine the communication protocol.
Likewise, we use imitation learning approaches to solve the problem of 
coordinating multiple agents, introducing a communication protocol, which 
consists in an explicit exchange of messages between the robots. The 
communication is not provided to the network, instead, it is a latent variable 
which has to be inferred.   
The results show the effectiveness of this communication strategy developed, as 
well as an illustration of the different patterns emerging from the tasks.
