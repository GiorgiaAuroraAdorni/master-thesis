\chapter{Literature review}
\label{chap:stateoftheart}
This chapter discusses previous research about the topic, providing a brief 
introduction to the approach we adopted.
%, evaluating, in particular, their shortcomings. 

\glsreset{ai}
\glsreset{dl}
\glsreset{rl} 
\glsreset{dnn}

\bigskip
In recent years, the application of \gls{ai} and \gls{dl} techniques to multi-agents 
cooperative problems has become increasingly successful.
Gasser and Huhns, in \emph{Distributed Artificial Intelligence} 
\cite[][]{gasser2014distributed}, have addressed the issue of coordination and 
cooperation among agents with the combination of distributed systems and 
\gls{ai}, a discipline known as \gls{dai}. 

Similarly, Panait and Luke, in their work \emph{Cooperative multi-agent learning: 
The state of the art} \cite[][]{panait2005cooperative}, report that \gls{mal}, the 
application of machine learning to problems involving multiple agents, has 
become a popular approach which deals with unusually large search spaces. 
In addition, they mention three of the most classic techniques used to solve 
cooperative \gls{ma} problems, that are Supervised and Unsupervised Learning, 
and \gls{rl}. These methods are distinguished according to the type of feedback 
provided, that in the first case is the target, in the second no feedback are 
provided, and in the last one is a reward of the learned output.

Of these three approaches, the vast majority of articles in this field used 
reward-based methods to approach \gls{ma} scenarios, in particular \gls{rl}, that 
make it possible to achieve sophisticated goals in complex and uncertain 
environments \cite[][]{oliehoek2012decentralised}. 
However, this technique is notoriously know for being hard, in particular it is 
difficult to design a suitable reward function for the agent to optimise, able to 
precisely lead to the desired behaviour in all possible scenarios. This problem is 
further exacerbated in multi-agent settings \cite[][]{hadfield2017inverse, 
oliehoek2012decentralised}.

\gls{irl} addresses this problem by using a given set of expert trajectories to 
produce a reward function under which these are optimal. 
Nevertheless, this technique imposes often unrealistic requirements 
\cite[][]{vsovsic2016inverse}.

\gls{il} is a class of methods that has been successfully applied to a wide range of 
domains in robotics, for example, the autonomous driving 
\cite[][]{schaal1999imitation, stepputtis2019imitation}. They aim to overcome the 
issues previously introduced, and unlike reward-based methods, the model 
acquires skills and provides actions to the agent observing the desired behaviour, 
performed by an expert \cite[][]{song2018multi, zhang2018deep, 
billard2008survey}.
Using this approach the models learn how to extract relevant information from 
the data provided to them, directly learning a mapping from observations to 
actions. 
A more challenging situation occurs when the model has to infer the coordination 
among the agents that is implicit in the demonstrations, using unsupervised 
approaches to imitation.

In this direction, literature suggests that cooperative tasks sometimes cannot be 
solve only using a simple distributed approach, instead it can be necessary to 
allow an explicit exchange of messages between the agents.
In \emph{Multi-agent reinforcement learning: Independent vs. cooperative 
agents} \cite[][]{tan1993multi}, Tan affirms that cooperating learners should use 
communication in a variety of ways in order to improve team performance: they 
can share instantaneous informations as well as episodic experience and learned 
knowledge.
Also Pesce and Montana propose the use of inter-agent communication for 
situations in which the agents can only acquire partial observations and are faced 
with a task requiring coordination and synchronisation skills 
\cite[][]{pesce2019improving}. Their solution consist in an explicit communication 
system that allows agents to exchange messages that are used together with local 
observations to decide which actions to take.

Our work is based on \emph{Learning distributed controllers by backpropagation}
\cite[][]{marcoverna2020}, which propose an approach in which at the same time 
are learned a distributed policy for the agents and a coordination model. 
This method proposed is based on an important concept introduced in 
\emph{Coordinated multi-agent imitation learning} \cite[][]{le2017coordinated}:  
the network has the ability to infer the communication protocol.
Likewise, we use imitation learning approaches to solve the problem of 
coordinating multiple agents, introducing a communication protocol, consisting 
in an explicit exchange of messages between the robots. The communication is 
not provided to the network, instead, it is a latent variable which has to be 
inferred.   
The results show the effectiveness of the communication strategy developed, as 
well as an illustration of the different patterns emerging from the tasks.
