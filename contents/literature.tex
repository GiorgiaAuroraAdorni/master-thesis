\chapter{Literature review}
\label{chap:stateoftheart}
This chapter discusses previous research about the topic, evaluating, in particular, 
their shortcomings. Finally, it provides a brief introduction to the approach we 
adopted.

\glsreset{ai}
\glsreset{dl}
\glsreset{rl} 
\glsreset{dnn}

In recent years, the application of \gls{ai} and \gls{dl} techniques to multi-agents 
cooperative problems has become increasingly successful.
Gasser and Huhns, in \emph{Distributed Artificial Intelligence} 
\cite[][]{gasser2014distributed}, have addressed the issue of coordination and 
cooperation among agents with the combination of distributed systems and 
\gls{ai}, a discipline known as \gls{dai}.

Panait and Luke, in their work \emph{Cooperative multi-agent learning: The state 
of the art} \cite[][]{panait2005cooperative}, report that \gls{mal}, the application 
of machine learning to problems involving multiple agents, has become a popular 
approach which deals with unusually large search spaces. 

Three classic techniques used to solve cooperative \gls{ma} problems are 
Supervised and Unsupervised Learning, and \gls{rl}, which are distinguished 
according to the type of feedback provided, that in the first case is the target, in 
the second no feedback are provided, and in the last one is a reward of the 
learned output \cite[][]{panait2005cooperative}.

The vast majority of articles in this field used reward-based methods, in particular, 
\gls{rl} approaches, that make it possible to achieve sophisticated goals in 
complex and uncertain environments \cite[][]{oliehoek2012decentralised}. 

However, in reward-based methods, it is notoriously hard to design a suitable 
reward function for the agent to optimise, able to precisely lead to the desired 
behaviour in all possible scenarios. This problem is further exacerbated in 
multi-agent settings \cite[][]{hadfield2017inverse, oliehoek2012decentralised}.

\gls{irl} addresses this problem by using a given set of expert trajectories to 
produce a reward function under which these are optimal. 
Nevertheless, this technique imposes often unrealistic requirements 
\cite[][]{vsovsic2016inverse}.

\gls{il} methods \cite[see][]{schaal1999imitation, stepputtis2019imitation} aim to 
overcome these issues via expert demonstrations \cite[][]{song2018multi}: by 
observing the desired behaviour, performed by an expert, the model acquires 
skills and provides actions to the agent \cite[][]{zhang2018deep, 
billard2008survey}.

Using this approach the models learn how to extract relevant information from 
the data provided to them, directly learning a mapping from observations to 
actions. 
A more challenging situation occurs when the model has to infer the coordination 
among the agents that is implicit in the demonstrations.

In \emph{Multi-agent reinforcement learning: Independent vs. cooperative 
agents} \cite[][]{tan1993multi}, Tan suggests that cooperating learners can use 
communication in a variety of ways in order to improve team performance. 

Also Pesce and Montana suggest the use of inter-agent communication for 
situations in which the agents can only acquire partial observations and are faced 
with a task requiring coordination and synchronisation skills 
\cite[][]{pesce2019improving}.

In this thesis work we use imitation learning approaches to solve the problem of 
coordinating multiple agents. Moreover, we introduce a communication protocol, 
consisting in an explicit exchange of messages between the robots, that is 
learned by the network in an unsupervised way as in \emph{Coordinated 
multi-agent imitation learning} \cite[][]{le2017coordinated}. 

The results show the effectiveness of the communication strategy developed, as 
well as an illustration of the different patterns emerging from the tasks.
