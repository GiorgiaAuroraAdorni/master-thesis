\section{Task 1}
\label{sec:task1}

\subsection{Description}
\label{subsec:desc1}

The first scenario tackle an interesting/newsworthy multi-agent coordination task.
As described in the overview of Chapter \ref{chap:experiments}, in which are 
provided additional details, in a \gls{1d} environment are spawn N robots in 
random positions in a "single file". %% FIXME within the proximity sensors range 

Each agent can update its state – its absolute position – by performing actions – 
moving forward and backward along the x-axis – based on the observations 
received from the environment – the distances from neighbours – using their 
sensors.

As a consequence of a \gls{1d}, the agents movement are limited to two 
directions: moving towards the x-axis when the velocity is positive, instead a 
negative one indicates a movement backward. 
The maximum speed allowed to the agent is 16.6 \gls{cm/s}.

The robots share a common goal: arrange themselves uniformly along the line 
between the two "dead" robots, in such a way they stand at equal distances from 
each other.

%% FIXME The problem presents a collaborative goal given the fact that the single 
%%agent can’t sense all the environment and the final configuration depends on 
%%the 
%%position of all of them.

The problem represent a cooperative goal that can be reached performing 
Imitation Learning, by following the example of an omniscient controller, 
introduced in Subsection \ref{subsec:omniscient}, through the training of an 
end-to-end neural network, that receives sensor inputs and produces 
commands for the motors.

The expert controller exploits its complete knowledge of the environment % FIXME 
%state of the system 
to decide the best action to perform.

In the course of this study, we tried to understand if its possible to use a controller 
learned by imitating an expert one, instead of using a manual controller.

\subsection{Experiments}
\label{subsec:ex1}

\subsection{Results}
\label{subsec:results1}

